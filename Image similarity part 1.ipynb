{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0a382-76a0-4cdd-8ea3-17599ecbe25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2  # For image processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Path to the label file and images folder\n",
    "label_file = 'D:\\Project\\identity_CelebA.txt'\n",
    "image_folder = 'D:\\Project\\Images'\n",
    "\n",
    "# Read the label file into a DataFrame\n",
    "labels_df = pd.read_csv(label_file, delim_whitespace=True, header=None, names=['image', 'celebrity_id'])\n",
    "\n",
    "# Append .jpg to each image name in the DataFrame\n",
    "labels_df['image'] = labels_df['image'].astype(str)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(labels_df.head())\n",
    "\n",
    "# Check if the images exist in the folder\n",
    "for image_name in labels_df['image']:\n",
    "    if not os.path.exists(os.path.join(image_folder, image_name)):\n",
    "        print(f\"Warning: {image_name} not found in {image_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de94fe-0b67-4e65-997e-44f2105263e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image size for resizing (e.g., 128x128 pixels)\n",
    "IMG_SIZE = 128\n",
    "\n",
    "def preprocess_images(image_folder, labels_df, img_size=128):\n",
    "    image_data = []\n",
    "    image_labels = []\n",
    "\n",
    "    for index, row in tqdm(labels_df.iterrows(), total=labels_df.shape[0]):\n",
    "        img_name = row['image']\n",
    "        img_path = os.path.join(image_folder, img_name)\n",
    "\n",
    "        # Read and process each image\n",
    "        try:\n",
    "            # Read the image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Resize the image to IMG_SIZE x IMG_SIZE\n",
    "                img = cv2.resize(img, (img_size, img_size))\n",
    "                \n",
    "                # Normalize pixel values to the range [0, 1]\n",
    "                img = img.astype('float32') / 255.0\n",
    "\n",
    "                # Append the image data and corresponding label\n",
    "                image_data.append(img)\n",
    "                image_labels.append(row['celebrity_id'])\n",
    "            else:\n",
    "                print(f\"Image {img_name} not found or corrupted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {e}\")\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    image_data = np.array(image_data)\n",
    "    image_labels = np.array(image_labels)\n",
    "\n",
    "    return image_data, image_labels\n",
    "\n",
    "# Preprocess the images\n",
    "image_data, image_labels = preprocess_images(image_folder, labels_df, IMG_SIZE)\n",
    "\n",
    "# Display the shape of the processed data\n",
    "print(f\"Processed image data shape: {image_data.shape}\")\n",
    "print(f\"Processed image labels shape: {image_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48d1e8-494e-4b41-85ea-d7047e08a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Define augmentation sequence\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),  # horizontal flips\n",
    "    iaa.Affine(rotate=(-25, 25)),  # rotate by -25 to 25 degrees\n",
    "    iaa.Multiply((0.8, 1.2)),  # change brightness\n",
    "    iaa.GaussianBlur(sigma=(0, 1.0)),  # Gaussian blur\n",
    "])\n",
    "\n",
    "# Augment images\n",
    "augmented_images = seq(images=image_data)  # assuming image_data is a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9894e-261f-4962-ba46-fbe492800eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "# Load the VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Prepare images for feature extraction\n",
    "def extract_features(img_array):\n",
    "    img_array = preprocess_input(img_array)  # preprocess for VGG16\n",
    "    features = model.predict(img_array)\n",
    "    return features\n",
    "\n",
    "# Extract features for the augmented images\n",
    "features = []\n",
    "for img in augmented_images:\n",
    "    feature_vector = extract_features(np.expand_dims(img, axis=0))\n",
    "    features.append(feature_vector)\n",
    "\n",
    "# Convert to NumPy array\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ef519-ab99-4a0b-8b32-f48849d58fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the array to remove the first dimension\n",
    "features_squeezed = features.squeeze(axis=1)  # This will change the shape to (5000, 4, 4, 512)\n",
    "\n",
    "# Now, flatten the feature vectors\n",
    "features_flattened = features_squeezed.reshape(len(features_squeezed), -1)  # This will reshape to (5000, 8192)\n",
    "\n",
    "# Check the shape of the flattened features\n",
    "print(features_flattened.shape)  # Should output (5000, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087552f0-1630-465d-9e1d-84000bfcb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, CollectionSchema, FieldSchema, DataType, Collection\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host='127.0.0.1', port='19530')\n",
    "\n",
    "feature_dim = 8192 \n",
    "\n",
    "# Define schema\n",
    "fields = [\n",
    "    FieldSchema(name='image_id', dtype=DataType.INT64, is_primary=True),\n",
    "    FieldSchema(name='feature_vector', dtype=DataType.FLOAT_VECTOR, dim=feature_dim),  # Specify the dimension of your feature vector\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields, description=\"Celeb Look-Alike Model\")\n",
    "\n",
    "# Create collection\n",
    "collection = Collection(name='celebrity_look_alike', schema=schema)\n",
    "\n",
    "# Insert features (each feature vector is already a 1D array with size feature_dim)\n",
    "# data = [\n",
    "#     [i for i in range(len(features_flattened))],  # Assuming you want to use indices as IDs\n",
    "#     [feature_vector for feature_vector in features]  # List of individual feature vectors\n",
    "# ]\n",
    "\n",
    "# # Ensure the feature vectors have the correct dimensions before inserting\n",
    "# collection.insert(data)\n",
    "\n",
    "# Insert features into Milvus\n",
    "# data = [\n",
    "#     [i for i in range(len(features_flattened))],  # Assuming you want to use indices as IDs\n",
    "#     features_flattened.tolist()  # Convert NumPy array to list for insertion\n",
    "# ]\n",
    "\n",
    "# collection.insert(data)\n",
    "BATCH_SIZE = 500  # Set a smaller batch size\n",
    "\n",
    "for start in range(0, len(features_flattened), BATCH_SIZE):\n",
    "    end = min(start + BATCH_SIZE, len(features_flattened))\n",
    "    batch_data = [\n",
    "        [i for i in range(start, end)],  # Use a range for IDs\n",
    "        features_flattened[start:end].tolist()  # Select the appropriate slice\n",
    "    ]\n",
    "    collection.insert(batch_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97b0fa-ed32-4c7c-bff7-2e45a950711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host='127.0.0.1', port='19530')\n",
    "\n",
    "# Step 2: Access the collection\n",
    "collection_name = \"celebrity_look_alike\"\n",
    "collection = Collection(collection_name)\n",
    "\n",
    "# Step 3: Get collection info\n",
    "collection_info = collection.num_entities  # Get number of entities in the collection\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Number of entities: {collection_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b7c5e-08be-4994-9f4a-f8170d262553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collection\n",
    "collection.load()\n",
    "\n",
    "# Check if the collection is loaded\n",
    "if collection.is_loaded:\n",
    "    print(\"Collection is loaded. Proceeding with search...\")\n",
    "\n",
    "    search_vector = features_flattened[0].tolist()  # Example feature vector for search\n",
    "    search_params = {\n",
    "        \"metric_type\": \"L2\",\n",
    "        \"params\": {\"nprobe\": 10}\n",
    "    }\n",
    "\n",
    "    # Perform the search\n",
    "    results = collection.search(data=[search_vector], anns_field='feature_vector', param=search_params, limit=10)\n",
    "\n",
    "    for result in results:\n",
    "        print(\"Search results:\", result)\n",
    "else:\n",
    "    print(\"Failed to load the collection.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
